{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName('flight_data_app')\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# data fetch\n",
    "# flight_data_2015 = (spark\n",
    "#                     .read\n",
    "#                     .option('inferSchema', 'true')\n",
    "#                     .option('header', 'true')\n",
    "#                     .csv(r'Spark-The-Definitive-Guide-master/data/flight-data/csv/2015-summary.csv')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', 5)\n",
    "\n",
    "flight_data_2015.sort('count').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.createOrReplaceTempView('flight_data_2015_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql('''\n",
    "SELECT DEST_COUNTRY_NAME, count(DEST_COUNTRY_NAME)\n",
    "FROM flight_data_2015_view\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay = (flight_data_2015\n",
    "                .groupBy('DEST_COUNTRY_NAME')\n",
    "                .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay.explain() == dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT MAX(count) FROM flight_data_2015_view').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|   Destination|Total_flights|\n",
      "+--------------+-------------+\n",
      "| United States|       411352|\n",
      "|        Canada|         8399|\n",
      "|        Mexico|         7140|\n",
      "|United Kingdom|         2025|\n",
      "|         Japan|         1548|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT \n",
    "        DEST_COUNTRY_NAME AS Destination, \n",
    "        SUM(count) AS Total_flights\n",
    "    FROM flight_data_2015_view\n",
    "    GROUP BY Destination\n",
    "    ORDER BY Total_flights DESC\n",
    "    LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = (spark\n",
    "                .read\n",
    "                .format('csv')\n",
    "                .option('inferSchema', 'true')\n",
    "                .option('header', 'true')\n",
    "                .load(r'Spark-The-Definitive-Guide-master\\data\\retail-data\\all\\online-retail-dataset.csv')\n",
    ")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView('retail_data')\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+\n",
      "|CustomerId|              window|sum(total_cost)|\n",
      "+----------+--------------------+---------------+\n",
      "|     13748|{2010-11-30 22:00...|          204.0|\n",
      "|     15291|{2010-11-30 22:00...|          328.8|\n",
      "|     17809|{2010-11-30 22:00...|           34.8|\n",
      "|     14527|{2010-11-30 22:00...|          -27.5|\n",
      "|     17420|{2010-11-30 22:00...|         130.85|\n",
      "+----------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, desc, col, to_date\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "(staticDataFrame\n",
    "        .withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"M/d/yyyy h:mm\"))\n",
    "        .selectExpr(\n",
    "            \"CustomerId\",\n",
    "            \"(UnitPrice * Quantity) as total_cost\",\n",
    "            \"InvoiceDate\")\n",
    "        .groupBy(\n",
    "            col(\"CustomerId\"), \n",
    "            window(col(\"InvoiceDate\"), \"1 day\")\n",
    "                )\n",
    "        .sum(\"total_cost\")\n",
    "        .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = (spark.readStream\n",
    "                        .schema(staticSchema)\n",
    "                        .option(\"maxFilesPerTrigger\", 1)\n",
    "                        .format(\"csv\")\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .load(r\"Spark-The-Definitive-Guide-master\\data\\retail-data\\by-day\\*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "preppedDataFrame = (staticDataFrame\n",
    "                        .na.fill(0)\n",
    "                        .withColumn(\"InvoiceDate\", F.to_date(F.col(\"InvoiceDate\"), \"M/d/yyyy h:mm\"))\n",
    "                        .withColumn(\"day_of_week\", F.date_format(F.col(\"InvoiceDate\"), \"EEEE\"))\n",
    "                        .coalesce(5)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "\n",
    "indexer = (StringIndexer()\n",
    "                .setInputCol(\"day_of_week\")\n",
    "                .setOutputCol(\"day_of_week_index\"))\n",
    "\n",
    "encoder = (OneHotEncoder()\n",
    "                .setInputCol(\"day_of_week_index\")\n",
    "                .setOutputCol(\"day_of_week_encoded\"))\n",
    "\n",
    "vectorAssembler = (VectorAssembler()\n",
    "                        .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\n",
    "                        .setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "transformationPipeline = (Pipeline().setStages([indexer, encoder, vectorAssembler]))\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette train with squared euclidean distance = 0.9518\n",
      "Silhouette test with squared euclidean distance = 0.9469\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "\n",
    "kmeans = (KMeans()\n",
    "            .setK(20)\n",
    "            .setSeed(1)\n",
    "        )\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "predictions_train = kmModel.transform(transformedTraining)\n",
    "predictions_test = kmModel.transform(transformedTest)\n",
    "\n",
    "silhouette_train = evaluator.evaluate(predictions_train)\n",
    "silhouette_test = evaluator.evaluate(predictions_test)\n",
    "\n",
    "print(f\"Silhouette train with squared euclidean distance = {silhouette_train:.4f}\")\n",
    "print(f\"Silhouette test with squared euclidean distance = {silhouette_test:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cff25e70c1e57fad421e91c4365585c5e52e605a4ac474950021b51ee0f23f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
